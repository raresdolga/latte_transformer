lr: 4.e-3
#train_steps: 10
batch_size: 64 # 90
epochs: 200
dataset_name: "pathfinder32" # "pathfinder128" # "imdb" #"aan"
conv_embed: False
pool: "mean"
weight_decay: 0.03
# dropout each layer
dropout: 0.2
batchnorm: False
prenorm: True
attention_type: "bid_latte" #"stable_latte"
block_type: "transformer"
hidden_dim: 256
nlayers: 6
nheads: 4 #
max_seq_len: 1024 # 4000 # 2000 #16384 # 
L: 256
unroll: 100
num_classes: 2
eval_steps: 0
project: "latte_lra"
entity: "baesian-learning"
wandb_log: True
disable_cache: False

