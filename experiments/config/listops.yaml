lr: 1.e-3 # 1.e-2 # 
lr_decay_fn: "cosine" # "constant"
#train_steps: 10
batch_size: 64
epochs: 50
pool: "mean"
weight_decay: 0.01 # 0.05
dataset_name: "listops" # "pathfinder128" # "imdb" #"aan"
dropout: 0.1
warmup_pc: 0
attention_type: "bid_latte" #"stable_latte"
block_type: "transformer"
batchnorm: False
prenorm: True
hidden_dim: 128
nlayers: 6
nheads: 4 #
L: 40
unroll: 100
max_seq_len: 2048
num_classes: 10
eval_steps: 0 # every iter
project: "latte_lra"
entity: "baesian-learning"
wandb_log: True
disable_cache: False
tokenizer_path: '/home/ubuntu/diffusion/data/tokenizers/tok_list_ops.json'

