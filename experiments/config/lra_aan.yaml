lr: 5.e-5
lr_decay_fn: "cosine" # "constant" #
batch_size: 32 # 8 # 16 # 90
#train_steps: 20000
warmup_pc: 0.08 #0.25 # results in approx 7354 warmup steps
epochs: 20
dataset_name: "aan" # "pathfinder128" # "imdb" #"aan"
pool: "last" # "CLS" #"last"
# dropout each layer
dropout: 0.1
weight_decay: 0.01
batchnorm: False
prenorm: True
attention_type: "bid_latte" # "stable_latte" # 
block_type: "transformer"
hidden_dim:  128 #256
nlayers: 4
nheads: 4 #
max_seq_len: 4000 #16384 # 4000 # 2000 # 1024
num_classes: 2
L: 40
unroll: 100
eval_steps: 2000
project: "latte_lra"
entity: "baesian-learning"
wandb_log: False # True
disable_cache: False